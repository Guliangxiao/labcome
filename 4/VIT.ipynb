{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, time\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "readvdnames = lambda x: open(x).read().rstrip().split('\\n')\n",
    "\n",
    "################################# DEFINE DATASET #################################\n",
    "class TinySegData(Dataset):\n",
    "    def __init__(self, db_root=\"TinySeg\", img_size=256, phase='train'):\n",
    "        classes = ['person', 'bird', 'car', 'cat', 'plane', ]\n",
    "        seg_ids = [1, 2, 3, 4, 5]\n",
    "\n",
    "        templ_image = db_root + \"/JPEGImages/{}.jpg\"\n",
    "        templ_mask = db_root + \"/Annotations/{}.png\"\n",
    "\n",
    "        ids = readvdnames(db_root + \"/ImageSets/\" + phase + \".txt\")\n",
    "\n",
    "        # build training and testing dbs\n",
    "        samples = []\n",
    "        for i in ids:\n",
    "            samples.append([templ_image.format(i), templ_mask.format(i)])\n",
    "        self.samples = samples\n",
    "        self.phase = phase\n",
    "        self.db_root = db_root\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.color_transform = torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2)\n",
    "\n",
    "        if not self.phase == 'train':\n",
    "            print (\"resize and augmentation will not be applied...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.phase == 'train':\n",
    "            return self.get_train_item(idx)\n",
    "        else:\n",
    "            return self.get_train_item(idx)\n",
    "\n",
    "    def get_train_item(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = Image.open(sample[0])\n",
    "\n",
    "        if random.randint(0, 1) > 0:\n",
    "            image = self.color_transform(image)\n",
    "        image = np.asarray(image)[..., ::-1]     # to BGR\n",
    "        seg_gt = (np.asarray(Image.open(sample[1]).convert('P'))).astype(np.uint8)\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        image = image / 127.5 - 1        # -1~1\n",
    "\n",
    "        if random.randint(0, 1) > 0:\n",
    "            image = image[:, ::-1, :]       # HWC\n",
    "            seg_gt = seg_gt[:, ::-1]\n",
    "\n",
    "        # random crop to 256x256\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        if height == width:\n",
    "            miny, maxy = 0, 256\n",
    "            minx, maxx = 0, 256\n",
    "        elif height > width:\n",
    "            miny = np.random.randint(0, height-256)\n",
    "            maxy = miny+256\n",
    "            minx = 0\n",
    "            maxx = 256\n",
    "        else:\n",
    "            miny = 0\n",
    "            maxy = 256\n",
    "            minx = np.random.randint(0, width-256)\n",
    "            maxx = minx+256\n",
    "        image = image[miny:maxy, minx:maxx, :].copy()\n",
    "        seg_gt = seg_gt[miny:maxy, minx:maxx].copy()\n",
    "\n",
    "        if self.img_size != 256:\n",
    "            new_size = (self.img_size, self.img_size)\n",
    "            image = cv2.resize(image, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "            seg_gt = cv2.resize(seg_gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))      # To CHW\n",
    "\n",
    "        # cv2.imwrite(\"test.png\", np.concatenate([(image[0]+1)*127.5, seg_gt*255], axis=0))\n",
    "        return image, seg_gt, sample\n",
    "\n",
    "    def get_test_item(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = cv2.imread(sample[0])\n",
    "        seg_gt = (np.asarray(Image.open(sample[1]).convert('P'))).astype(np.uint8)\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        image = image / 127.5 - 1        # -1~1\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # cv2.imwrite(\"test.png\", np.concatenate([(image[0]+1)*127.5, seg_gt*255], axis=0))\n",
    "        return image, seg_gt, sample\n",
    "\n",
    "################################# FUNCTIONS #################################\n",
    "def get_confusion_matrix(gt_label, pred_label, class_num):\n",
    "        \"\"\"\n",
    "        Calcute the confusion matrix by given label and pred\n",
    "        :param gt_label: the ground truth label\n",
    "        :param pred_label: the pred label\n",
    "        :param class_num: the number of class\n",
    "        :return: the confusion matrix\n",
    "        \"\"\"\n",
    "        index = (gt_label * class_num + pred_label).astype('int32')\n",
    "\n",
    "        label_count = np.bincount(index)\n",
    "        confusion_matrix = np.zeros((class_num, class_num))\n",
    "\n",
    "        for i_label in range(class_num):\n",
    "            for i_pred_label in range(class_num):\n",
    "                cur_index = i_label * class_num + i_pred_label\n",
    "                if cur_index < len(label_count):\n",
    "                    confusion_matrix[i_label, i_pred_label] = label_count[cur_index]\n",
    "\n",
    "        return confusion_matrix\n",
    "\n",
    "def get_confusion_matrix_for_3d(gt_label, pred_label, class_num):\n",
    "    confusion_matrix = np.zeros((class_num, class_num))\n",
    "\n",
    "    for sub_gt_label, sub_pred_label in zip(gt_label, pred_label):\n",
    "        sub_gt_label = sub_gt_label[sub_gt_label != 255]\n",
    "        sub_pred_label = sub_pred_label[sub_pred_label != 255]\n",
    "        cm = get_confusion_matrix(sub_gt_label, sub_pred_label, class_num)\n",
    "        confusion_matrix += cm\n",
    "    return confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):  \n",
    "    def __init__(self, embed_size = 3*16*16 , heads = 1, dropout=0.2):  \n",
    "        super(Attention, self).__init__()  \n",
    "        self.embed_size = embed_size  \n",
    "        self.heads = heads  \n",
    "        self.head_dim = embed_size // heads  \n",
    "\n",
    "        assert (  \n",
    "            self.head_dim * heads == embed_size  \n",
    "        ), \"Embedding size must be divisible by heads\"  \n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)  \n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)  \n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)  \n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)  \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):    \n",
    "\n",
    "        values = self.values(x)  # (batch_size, seq_length, embed_size)  \n",
    "        keys = self.keys(x)      \n",
    "        queries = self.queries(x)  \n",
    "\n",
    "       \n",
    "        # Scaled dot-product attention  \n",
    "        \n",
    "        energy = torch.bmm(queries, keys.transpose(-2, -1))  # (batch_size, query_length, key_length)  \n",
    "        \n",
    "\n",
    "        # 进行缩放  \n",
    "        scaling = self.head_dim ** 0.5  \n",
    "        scaled_energy = energy / scaling  # (batch_size, query_length, key_length)  \n",
    "\n",
    "        # 应用 softmax  \n",
    "        attention = nn.functional.softmax(scaled_energy, dim=-1)\n",
    "\n",
    "        # 计算输出  \n",
    "        out = torch.bmm(attention, values)  # (batch_size, query_length, head_dim)  \n",
    "        # 注意：这里的 values 仍然是 (batch_size, heads, seq_length, head_dim)  \n",
    "\n",
    "    \n",
    "\n",
    "        # 最后，通过线性层输出  \n",
    "        out = self.fc_out(out)  \n",
    "        return self.dropout(out)  \n",
    "    \n",
    "class VisionEncoder(nn.Module):  \n",
    "    def __init__(self, embed_size = 3*16*16, heads = 1, drop_rate=0.2):  \n",
    "        super(VisionEncoder, self).__init__()  \n",
    "        self.attention = Attention(embed_size, heads, dropout=drop_rate)  \n",
    "        self.norm1 = nn.LayerNorm(embed_size)  \n",
    "        self.norm2 = nn.LayerNorm(embed_size)  \n",
    "        self.mlp = nn.Sequential(  \n",
    "            nn.Linear(embed_size, 2048),  \n",
    "            nn.GELU(),  \n",
    "            nn.Dropout(drop_rate),  \n",
    "            nn.Linear(2048, embed_size),  \n",
    "            nn.Dropout(drop_rate)  \n",
    "        )  \n",
    "\n",
    "    def forward(self, x):  \n",
    "        attention = self.attention(x)  \n",
    "        x = self.norm1(attention + x)  # Residual Connection  \n",
    "        mlp_out = self.mlp(x)  \n",
    "        x = self.norm2(mlp_out + x)  # Residual Connection  \n",
    "        return x  \n",
    "\n",
    "class VisionTransformer(nn.Module):  \n",
    "    def __init__(self, num_classes = 20, embed_size=3*16*16, num_layers=3, heads=1, num_patches=14*14, drop_rate=0.2):  \n",
    "        super(VisionTransformer, self).__init__()  \n",
    "        self.patch = nn.Conv2d(3,embed_size,16,16)\n",
    "        self.encoders = nn.ModuleList(  \n",
    "            [VisionEncoder(embed_size, heads, drop_rate) for _ in range(num_layers)]  \n",
    "        )  \n",
    "        self.norm = nn.LayerNorm(embed_size)  \n",
    "        self.classifier = nn.Linear(embed_size, num_classes)  \n",
    "        self.dropout = nn.Dropout(drop_rate)  \n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.patch(x)\n",
    "        x = x.flatten(2).permute(0,2,1)\n",
    "        \n",
    "        for encoder in self.encoders:  \n",
    "            x = encoder(x)  \n",
    "        \n",
    "        x = self.norm(x)  \n",
    "        x = self.dropout(x)  \n",
    "        x = x.mean(dim=1)  # Global average pooling  \n",
    "        return self.classifier(x)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resize and augmentation will not be applied...\n"
     ]
    }
   ],
   "source": [
    "dataset = TinySegData(img_size=224, phase='train')\n",
    "test = TinySegData(img_size=224,phase='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=64)\n",
    "test_loader = DataLoader(test,batch_size=32)\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, Train Loss: 0.128710, Train Acc: 0.506926\n",
      "epoch: 2, Train Loss: 0.102692, Train Acc: 0.530197\n",
      "epoch: 3, Train Loss: 0.097559, Train Acc: 0.555519\n",
      "epoch: 4, Train Loss: 0.093686, Train Acc: 0.572695\n",
      "epoch: 5, Train Loss: 0.091486, Train Acc: 0.583112\n",
      "epoch: 6, Train Loss: 0.090196, Train Acc: 0.600399\n",
      "epoch: 7, Train Loss: 0.087856, Train Acc: 0.604998\n",
      "epoch: 8, Train Loss: 0.087754, Train Acc: 0.611536\n",
      "epoch: 9, Train Loss: 0.086006, Train Acc: 0.614251\n",
      "epoch: 10, Train Loss: 0.085187, Train Acc: 0.626496\n",
      "epoch: 11, Train Loss: 0.084372, Train Acc: 0.627770\n",
      "epoch: 12, Train Loss: 0.084548, Train Acc: 0.630762\n",
      "epoch: 13, Train Loss: 0.083482, Train Acc: 0.625388\n",
      "epoch: 14, Train Loss: 0.083114, Train Acc: 0.626441\n",
      "epoch: 15, Train Loss: 0.081754, Train Acc: 0.637467\n",
      "epoch: 16, Train Loss: 0.081868, Train Acc: 0.643562\n",
      "epoch: 17, Train Loss: 0.080822, Train Acc: 0.642841\n",
      "epoch: 18, Train Loss: 0.081486, Train Acc: 0.640403\n",
      "epoch: 19, Train Loss: 0.080384, Train Acc: 0.647717\n",
      "epoch: 20, Train Loss: 0.079907, Train Acc: 0.650266\n",
      "epoch: 21, Train Loss: 0.079743, Train Acc: 0.651263\n",
      "epoch: 22, Train Loss: 0.078614, Train Acc: 0.652648\n",
      "epoch: 23, Train Loss: 0.079030, Train Acc: 0.652926\n",
      "epoch: 24, Train Loss: 0.078310, Train Acc: 0.650931\n",
      "epoch: 25, Train Loss: 0.078508, Train Acc: 0.652039\n",
      "epoch: 26, Train Loss: 0.078521, Train Acc: 0.652926\n",
      "epoch: 27, Train Loss: 0.077615, Train Acc: 0.657691\n",
      "epoch: 28, Train Loss: 0.076071, Train Acc: 0.665946\n",
      "epoch: 29, Train Loss: 0.076892, Train Acc: 0.667165\n",
      "epoch: 30, Train Loss: 0.076245, Train Acc: 0.669271\n",
      "epoch: 31, Train Loss: 0.075445, Train Acc: 0.672762\n",
      "epoch: 32, Train Loss: 0.076025, Train Acc: 0.666667\n",
      "epoch: 33, Train Loss: 0.075464, Train Acc: 0.671376\n",
      "epoch: 34, Train Loss: 0.074288, Train Acc: 0.678746\n",
      "epoch: 35, Train Loss: 0.074137, Train Acc: 0.675144\n",
      "epoch: 36, Train Loss: 0.073946, Train Acc: 0.676031\n",
      "epoch: 37, Train Loss: 0.074220, Train Acc: 0.677139\n",
      "epoch: 38, Train Loss: 0.072583, Train Acc: 0.686558\n",
      "epoch: 39, Train Loss: 0.072877, Train Acc: 0.685838\n",
      "epoch: 40, Train Loss: 0.072320, Train Acc: 0.690824\n",
      "epoch: 41, Train Loss: 0.071961, Train Acc: 0.684951\n",
      "epoch: 42, Train Loss: 0.071270, Train Acc: 0.691323\n",
      "epoch: 43, Train Loss: 0.070600, Train Acc: 0.699745\n",
      "epoch: 44, Train Loss: 0.071316, Train Acc: 0.691656\n",
      "epoch: 45, Train Loss: 0.070459, Train Acc: 0.702848\n",
      "epoch: 46, Train Loss: 0.070660, Train Acc: 0.695700\n",
      "epoch: 47, Train Loss: 0.069806, Train Acc: 0.703291\n",
      "epoch: 48, Train Loss: 0.069111, Train Acc: 0.706560\n",
      "epoch: 49, Train Loss: 0.069503, Train Acc: 0.705508\n",
      "epoch: 50, Train Loss: 0.068722, Train Acc: 0.707613\n",
      "epoch: 51, Train Loss: 0.069080, Train Acc: 0.703457\n",
      "epoch: 52, Train Loss: 0.068434, Train Acc: 0.704289\n",
      "epoch: 53, Train Loss: 0.067184, Train Acc: 0.708943\n",
      "epoch: 54, Train Loss: 0.067470, Train Acc: 0.708444\n",
      "epoch: 55, Train Loss: 0.066809, Train Acc: 0.710882\n",
      "epoch: 56, Train Loss: 0.066640, Train Acc: 0.718861\n",
      "epoch: 57, Train Loss: 0.065592, Train Acc: 0.724512\n",
      "epoch: 58, Train Loss: 0.065606, Train Acc: 0.720800\n",
      "epoch: 59, Train Loss: 0.064863, Train Acc: 0.720080\n",
      "epoch: 60, Train Loss: 0.064732, Train Acc: 0.721354\n",
      "epoch: 61, Train Loss: 0.063737, Train Acc: 0.731383\n",
      "epoch: 62, Train Loss: 0.065002, Train Acc: 0.724568\n",
      "epoch: 63, Train Loss: 0.064504, Train Acc: 0.732214\n",
      "epoch: 64, Train Loss: 0.063266, Train Acc: 0.732990\n",
      "epoch: 65, Train Loss: 0.063591, Train Acc: 0.734652\n",
      "epoch: 66, Train Loss: 0.063118, Train Acc: 0.736758\n",
      "epoch: 67, Train Loss: 0.062409, Train Acc: 0.738420\n",
      "epoch: 68, Train Loss: 0.062261, Train Acc: 0.738087\n",
      "epoch: 69, Train Loss: 0.062239, Train Acc: 0.742686\n",
      "epoch: 70, Train Loss: 0.062679, Train Acc: 0.739362\n",
      "epoch: 71, Train Loss: 0.062512, Train Acc: 0.740470\n",
      "epoch: 72, Train Loss: 0.062063, Train Acc: 0.739085\n",
      "epoch: 73, Train Loss: 0.061672, Train Acc: 0.739473\n",
      "epoch: 74, Train Loss: 0.061393, Train Acc: 0.745789\n",
      "epoch: 75, Train Loss: 0.061624, Train Acc: 0.739306\n",
      "epoch: 76, Train Loss: 0.061780, Train Acc: 0.736314\n",
      "epoch: 77, Train Loss: 0.061225, Train Acc: 0.746897\n",
      "epoch: 78, Train Loss: 0.061189, Train Acc: 0.744293\n",
      "epoch: 79, Train Loss: 0.060995, Train Acc: 0.743074\n",
      "epoch: 80, Train Loss: 0.061212, Train Acc: 0.746288\n",
      "epoch: 81, Train Loss: 0.060618, Train Acc: 0.742354\n",
      "epoch: 82, Train Loss: 0.060964, Train Acc: 0.748005\n",
      "epoch: 83, Train Loss: 0.060243, Train Acc: 0.741523\n",
      "epoch: 84, Train Loss: 0.060457, Train Acc: 0.746786\n",
      "epoch: 85, Train Loss: 0.060050, Train Acc: 0.752383\n",
      "epoch: 86, Train Loss: 0.059296, Train Acc: 0.755042\n",
      "epoch: 87, Train Loss: 0.060545, Train Acc: 0.748227\n",
      "epoch: 88, Train Loss: 0.058972, Train Acc: 0.757203\n",
      "epoch: 89, Train Loss: 0.058182, Train Acc: 0.761802\n",
      "epoch: 90, Train Loss: 0.058758, Train Acc: 0.757812\n",
      "epoch: 91, Train Loss: 0.058894, Train Acc: 0.755430\n",
      "epoch: 92, Train Loss: 0.059182, Train Acc: 0.753324\n",
      "epoch: 93, Train Loss: 0.059019, Train Acc: 0.753047\n",
      "epoch: 94, Train Loss: 0.058245, Train Acc: 0.759586\n",
      "epoch: 95, Train Loss: 0.057698, Train Acc: 0.759752\n",
      "epoch: 96, Train Loss: 0.058184, Train Acc: 0.762522\n",
      "epoch: 97, Train Loss: 0.057988, Train Acc: 0.761192\n",
      "epoch: 98, Train Loss: 0.056868, Train Acc: 0.768063\n",
      "epoch: 99, Train Loss: 0.057978, Train Acc: 0.765847\n",
      "epoch: 100, Train Loss: 0.057741, Train Acc: 0.762744\n",
      "epoch: 101, Train Loss: 0.057796, Train Acc: 0.763741\n",
      "epoch: 102, Train Loss: 0.057726, Train Acc: 0.759752\n",
      "epoch: 103, Train Loss: 0.057831, Train Acc: 0.760694\n",
      "epoch: 104, Train Loss: 0.057807, Train Acc: 0.762356\n",
      "epoch: 105, Train Loss: 0.057853, Train Acc: 0.764295\n",
      "epoch: 106, Train Loss: 0.057114, Train Acc: 0.762965\n",
      "epoch: 107, Train Loss: 0.057887, Train Acc: 0.757314\n",
      "epoch: 108, Train Loss: 0.058909, Train Acc: 0.753324\n",
      "epoch: 109, Train Loss: 0.057809, Train Acc: 0.756704\n",
      "epoch: 110, Train Loss: 0.058170, Train Acc: 0.756760\n",
      "epoch: 111, Train Loss: 0.058625, Train Acc: 0.758311\n",
      "epoch: 112, Train Loss: 0.059006, Train Acc: 0.760472\n",
      "epoch: 113, Train Loss: 0.057690, Train Acc: 0.758145\n",
      "epoch: 114, Train Loss: 0.058633, Train Acc: 0.760417\n",
      "epoch: 115, Train Loss: 0.058844, Train Acc: 0.752715\n",
      "epoch: 116, Train Loss: 0.059109, Train Acc: 0.753879\n",
      "epoch: 117, Train Loss: 0.058607, Train Acc: 0.758644\n",
      "epoch: 118, Train Loss: 0.059495, Train Acc: 0.755208\n",
      "epoch: 119, Train Loss: 0.058496, Train Acc: 0.754543\n",
      "epoch: 120, Train Loss: 0.058446, Train Acc: 0.754156\n",
      "epoch: 121, Train Loss: 0.058320, Train Acc: 0.760749\n",
      "epoch: 122, Train Loss: 0.059247, Train Acc: 0.757480\n",
      "epoch: 123, Train Loss: 0.058986, Train Acc: 0.759475\n",
      "epoch: 124, Train Loss: 0.058837, Train Acc: 0.754599\n",
      "epoch: 125, Train Loss: 0.059469, Train Acc: 0.753158\n",
      "epoch: 126, Train Loss: 0.058224, Train Acc: 0.762079\n",
      "epoch: 127, Train Loss: 0.058642, Train Acc: 0.756483\n",
      "epoch: 128, Train Loss: 0.058738, Train Acc: 0.753712\n",
      "epoch: 129, Train Loss: 0.058229, Train Acc: 0.759807\n",
      "epoch: 130, Train Loss: 0.057757, Train Acc: 0.761414\n",
      "epoch: 131, Train Loss: 0.057043, Train Acc: 0.767841\n",
      "epoch: 132, Train Loss: 0.057755, Train Acc: 0.764461\n",
      "epoch: 133, Train Loss: 0.058100, Train Acc: 0.760306\n",
      "epoch: 134, Train Loss: 0.057496, Train Acc: 0.765126\n",
      "epoch: 135, Train Loss: 0.058084, Train Acc: 0.764572\n",
      "epoch: 136, Train Loss: 0.058406, Train Acc: 0.761636\n",
      "epoch: 137, Train Loss: 0.058350, Train Acc: 0.760915\n",
      "epoch: 138, Train Loss: 0.057230, Train Acc: 0.767620\n",
      "epoch: 139, Train Loss: 0.057937, Train Acc: 0.759198\n",
      "epoch: 140, Train Loss: 0.057635, Train Acc: 0.759807\n",
      "epoch: 141, Train Loss: 0.057379, Train Acc: 0.761857\n",
      "epoch: 142, Train Loss: 0.057676, Train Acc: 0.766512\n",
      "epoch: 143, Train Loss: 0.057719, Train Acc: 0.762744\n",
      "epoch: 144, Train Loss: 0.058228, Train Acc: 0.761414\n",
      "epoch: 145, Train Loss: 0.057335, Train Acc: 0.766567\n",
      "epoch: 146, Train Loss: 0.057689, Train Acc: 0.755042\n",
      "epoch: 147, Train Loss: 0.057675, Train Acc: 0.761469\n",
      "epoch: 148, Train Loss: 0.057186, Train Acc: 0.766068\n",
      "epoch: 149, Train Loss: 0.057019, Train Acc: 0.765625\n",
      "epoch: 150, Train Loss: 0.057830, Train Acc: 0.761913\n",
      "epoch: 151, Train Loss: 0.058054, Train Acc: 0.757812\n",
      "epoch: 152, Train Loss: 0.057893, Train Acc: 0.759586\n",
      "epoch: 153, Train Loss: 0.058220, Train Acc: 0.759641\n",
      "epoch: 154, Train Loss: 0.057492, Train Acc: 0.765847\n",
      "epoch: 155, Train Loss: 0.057072, Train Acc: 0.769116\n",
      "epoch: 156, Train Loss: 0.057740, Train Acc: 0.763520\n",
      "epoch: 157, Train Loss: 0.058955, Train Acc: 0.749612\n",
      "epoch: 158, Train Loss: 0.057413, Train Acc: 0.767453\n",
      "epoch: 159, Train Loss: 0.057819, Train Acc: 0.757480\n",
      "epoch: 160, Train Loss: 0.056824, Train Acc: 0.761968\n",
      "epoch: 161, Train Loss: 0.057483, Train Acc: 0.758533\n",
      "epoch: 162, Train Loss: 0.056613, Train Acc: 0.768617\n",
      "epoch: 163, Train Loss: 0.058484, Train Acc: 0.755208\n",
      "epoch: 164, Train Loss: 0.057755, Train Acc: 0.761691\n",
      "epoch: 165, Train Loss: 0.058058, Train Acc: 0.757591\n",
      "epoch: 166, Train Loss: 0.057482, Train Acc: 0.763409\n",
      "epoch: 167, Train Loss: 0.057255, Train Acc: 0.757702\n",
      "epoch: 168, Train Loss: 0.058002, Train Acc: 0.758422\n",
      "epoch: 169, Train Loss: 0.057181, Train Acc: 0.761802\n",
      "epoch: 170, Train Loss: 0.057378, Train Acc: 0.759087\n",
      "epoch: 171, Train Loss: 0.058066, Train Acc: 0.758422\n",
      "epoch: 172, Train Loss: 0.057323, Train Acc: 0.760915\n",
      "epoch: 173, Train Loss: 0.056661, Train Acc: 0.768562\n",
      "epoch: 174, Train Loss: 0.057510, Train Acc: 0.761414\n",
      "epoch: 175, Train Loss: 0.057740, Train Acc: 0.758865\n",
      "epoch: 176, Train Loss: 0.056806, Train Acc: 0.763797\n",
      "epoch: 177, Train Loss: 0.057630, Train Acc: 0.762910\n",
      "epoch: 178, Train Loss: 0.057619, Train Acc: 0.761802\n",
      "epoch: 179, Train Loss: 0.057067, Train Acc: 0.764295\n",
      "epoch: 180, Train Loss: 0.055578, Train Acc: 0.775543\n",
      "epoch: 181, Train Loss: 0.057349, Train Acc: 0.763963\n",
      "epoch: 182, Train Loss: 0.056581, Train Acc: 0.766622\n",
      "epoch: 183, Train Loss: 0.057536, Train Acc: 0.759031\n",
      "epoch: 184, Train Loss: 0.056388, Train Acc: 0.771941\n",
      "epoch: 185, Train Loss: 0.055204, Train Acc: 0.775100\n",
      "epoch: 186, Train Loss: 0.057282, Train Acc: 0.764960\n",
      "epoch: 187, Train Loss: 0.056263, Train Acc: 0.766789\n",
      "epoch: 188, Train Loss: 0.054546, Train Acc: 0.779200\n",
      "epoch: 189, Train Loss: 0.055708, Train Acc: 0.778203\n",
      "epoch: 190, Train Loss: 0.056240, Train Acc: 0.768506\n",
      "epoch: 191, Train Loss: 0.055279, Train Acc: 0.766401\n",
      "epoch: 192, Train Loss: 0.056383, Train Acc: 0.766844\n",
      "epoch: 193, Train Loss: 0.054929, Train Acc: 0.773438\n",
      "epoch: 194, Train Loss: 0.054684, Train Acc: 0.774435\n",
      "epoch: 195, Train Loss: 0.056415, Train Acc: 0.768008\n",
      "epoch: 196, Train Loss: 0.052665, Train Acc: 0.784131\n",
      "epoch: 197, Train Loss: 0.053452, Train Acc: 0.781804\n",
      "epoch: 198, Train Loss: 0.055193, Train Acc: 0.779588\n",
      "epoch: 199, Train Loss: 0.055131, Train Acc: 0.771664\n",
      "epoch: 200, Train Loss: 0.053263, Train Acc: 0.781582\n"
     ]
    }
   ],
   "source": [
    "example = VisionTransformer().to(device)\n",
    "\n",
    "lr = 0.0001\n",
    "epoches = 40\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(example.parameters(),lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)  \n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "loss_list1 = []\n",
    "accuracy_list1 = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for img,label,_ in train_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = example(img)\n",
    "        loss = criterion(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        train_acc += acc\n",
    "    scheduler.step()  \n",
    "\n",
    "    loss_list.append(train_loss/len(train_loader))\n",
    "    accuracy_list.append(train_acc/len(train_loader))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}'.format(epoch+1, train_loss/len(train_loader), train_acc/len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6703125\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "for img,label,_ in test_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = example(img)\n",
    "\n",
    "\n",
    "\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        test_acc += acc\n",
    "print(test_acc/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-cv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
