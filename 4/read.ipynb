{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, time\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "readvdnames = lambda x: open(x).read().rstrip().split('\\n')\n",
    "\n",
    "################################# DEFINE DATASET #################################\n",
    "class TinySegData(Dataset):\n",
    "    def __init__(self, db_root=\"TinySeg\", img_size=256, phase='train'):\n",
    "        classes = ['person', 'bird', 'car', 'cat', 'plane', ]\n",
    "        seg_ids = [1, 2, 3, 4, 5]\n",
    "\n",
    "        templ_image = db_root + \"/JPEGImages/{}.jpg\"\n",
    "        templ_mask = db_root + \"/Annotations/{}.png\"\n",
    "\n",
    "        ids = readvdnames(db_root + \"/ImageSets/\" + phase + \".txt\")\n",
    "\n",
    "        # build training and testing dbs\n",
    "        samples = []\n",
    "        for i in ids:\n",
    "            samples.append([templ_image.format(i), templ_mask.format(i)])\n",
    "        self.samples = samples\n",
    "        self.phase = phase\n",
    "        self.db_root = db_root\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.color_transform = torchvision.transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2)\n",
    "\n",
    "        if not self.phase == 'train':\n",
    "            print (\"resize and augmentation will not be applied...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.phase == 'train':\n",
    "            return self.get_train_item(idx)\n",
    "        else:\n",
    "            return self.get_train_item(idx)\n",
    "\n",
    "    def get_train_item(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = Image.open(sample[0])\n",
    "\n",
    "        if random.randint(0, 1) > 0:\n",
    "            image = self.color_transform(image)\n",
    "        image = np.asarray(image)[..., ::-1]     # to BGR\n",
    "        seg_gt = (np.asarray(Image.open(sample[1]).convert('P'))).astype(np.uint8)\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        image = image / 127.5 - 1        # -1~1\n",
    "\n",
    "        if random.randint(0, 1) > 0:\n",
    "            image = image[:, ::-1, :]       # HWC\n",
    "            seg_gt = seg_gt[:, ::-1]\n",
    "\n",
    "        # random crop to 256x256\n",
    "        height, width = image.shape[0], image.shape[1]\n",
    "        if height == width:\n",
    "            miny, maxy = 0, 256\n",
    "            minx, maxx = 0, 256\n",
    "        elif height > width:\n",
    "            miny = np.random.randint(0, height-256)\n",
    "            maxy = miny+256\n",
    "            minx = 0\n",
    "            maxx = 256\n",
    "        else:\n",
    "            miny = 0\n",
    "            maxy = 256\n",
    "            minx = np.random.randint(0, width-256)\n",
    "            maxx = minx+256\n",
    "        image = image[miny:maxy, minx:maxx, :].copy()\n",
    "        seg_gt = seg_gt[miny:maxy, minx:maxx].copy()\n",
    "\n",
    "        if self.img_size != 256:\n",
    "            new_size = (self.img_size, self.img_size)\n",
    "            image = cv2.resize(image, new_size, interpolation=cv2.INTER_LINEAR)\n",
    "            seg_gt = cv2.resize(seg_gt, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))      # To CHW\n",
    "\n",
    "        # cv2.imwrite(\"test.png\", np.concatenate([(image[0]+1)*127.5, seg_gt*255], axis=0))\n",
    "        return image, seg_gt, sample\n",
    "\n",
    "    def get_test_item(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        image = cv2.imread(sample[0])\n",
    "        seg_gt = (np.asarray(Image.open(sample[1]).convert('P'))).astype(np.uint8)\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        image = image / 127.5 - 1        # -1~1\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # cv2.imwrite(\"test.png\", np.concatenate([(image[0]+1)*127.5, seg_gt*255], axis=0))\n",
    "        return image, seg_gt, sample\n",
    "\n",
    "################################# FUNCTIONS #################################\n",
    "def get_confusion_matrix(gt_label, pred_label, class_num):\n",
    "        \"\"\"\n",
    "        Calcute the confusion matrix by given label and pred\n",
    "        :param gt_label: the ground truth label\n",
    "        :param pred_label: the pred label\n",
    "        :param class_num: the number of class\n",
    "        :return: the confusion matrix\n",
    "        \"\"\"\n",
    "        index = (gt_label * class_num + pred_label).astype('int32')\n",
    "\n",
    "        label_count = np.bincount(index)\n",
    "        confusion_matrix = np.zeros((class_num, class_num))\n",
    "\n",
    "        for i_label in range(class_num):\n",
    "            for i_pred_label in range(class_num):\n",
    "                cur_index = i_label * class_num + i_pred_label\n",
    "                if cur_index < len(label_count):\n",
    "                    confusion_matrix[i_label, i_pred_label] = label_count[cur_index]\n",
    "\n",
    "        return confusion_matrix\n",
    "\n",
    "def get_confusion_matrix_for_3d(gt_label, pred_label, class_num):\n",
    "    confusion_matrix = np.zeros((class_num, class_num))\n",
    "\n",
    "    for sub_gt_label, sub_pred_label in zip(gt_label, pred_label):\n",
    "        sub_gt_label = sub_gt_label[sub_gt_label != 255]\n",
    "        sub_pred_label = sub_pred_label[sub_pred_label != 255]\n",
    "        cm = get_confusion_matrix(sub_gt_label, sub_pred_label, class_num)\n",
    "        confusion_matrix += cm\n",
    "    return confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resize and augmentation will not be applied...\n",
      "resize and augmentation will not be applied...\n"
     ]
    }
   ],
   "source": [
    "dataset = TinySegData(img_size=128, phase='train')\n",
    "test = TinySegData(img_size=128,phase='val')\n",
    "resnetdataset = TinySegData(img_size=224, phase='train')\n",
    "resnettest = TinySegData(img_size=224,phase='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset, batch_size=32)\n",
    "test_loader = DataLoader(test,batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        # 定义网络层  \n",
    "        self.conv1 = nn.Conv2d(3, 18, kernel_size=21)  \n",
    "        self.conv2 = nn.Conv2d(18, 48, kernel_size=21)  \n",
    "        self.fc1 = nn.Linear(48 * 17 * 17, 360)  \n",
    "        self.fc2 = nn.Linear(360, 84)  \n",
    "        self.fc3 = nn.Linear(84, 20)  \n",
    "\n",
    "    def forward(self, x):  \n",
    "        # 定义前向传播过程  \n",
    "        x = nn.functional.relu(self.conv1(x))  \n",
    "        x = nn.functional.max_pool2d(x, 2)  \n",
    "        x = nn.functional.relu(self.conv2(x))  \n",
    "        x = nn.functional.max_pool2d(x, 2)  \n",
    "        x = x.view(-1, 48 * 17 * 17)  # flatten  \n",
    "        x = nn.functional.relu(self.fc1(x))  \n",
    "        x = nn.functional.relu(self.fc2(x))  \n",
    "        x = self.fc3(x)  \n",
    "        return x  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(vgg16,self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64,64,3,padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),#这里使用两个3X3的卷积核代替5X5\n",
    "\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  \n",
    "\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "            nn.Conv2d(1024, 2048, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(2048, 2048, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "\n",
    "            nn.Conv2d(2048, 4096, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(4096, 4096, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.Conv2d(4096, 4096, kernel_size=3, padding=1),  \n",
    "            nn.GELU(),  \n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4096 * 1 * 1, 4096),  \n",
    "            nn.GELU(),  \n",
    "            nn.Dropout(),  \n",
    "            nn.Linear(4096, 4096),  \n",
    "            nn.GELU(),  \n",
    "            nn.Dropout(),  #防过拟合\n",
    "            nn.Linear(4096, 20),  \n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, Train Loss: 0.690297, Train Acc: 0.059176\n",
      "epoch: 2, Train Loss: 0.672497, Train Acc: 0.010472\n",
      "epoch: 3, Train Loss: 0.552837, Train Acc: 0.130818\n",
      "epoch: 4, Train Loss: 0.238769, Train Acc: 0.306017\n",
      "epoch: 5, Train Loss: 0.144769, Train Acc: 0.511802\n",
      "epoch: 6, Train Loss: 0.126908, Train Acc: 0.513298\n",
      "epoch: 7, Train Loss: 0.120437, Train Acc: 0.510140\n",
      "epoch: 8, Train Loss: 0.117039, Train Acc: 0.510971\n",
      "epoch: 9, Train Loss: 0.115428, Train Acc: 0.509973\n",
      "epoch: 10, Train Loss: 0.113891, Train Acc: 0.512965\n",
      "epoch: 11, Train Loss: 0.112842, Train Acc: 0.512301\n",
      "epoch: 12, Train Loss: 0.112729, Train Acc: 0.512467\n",
      "epoch: 13, Train Loss: 0.111668, Train Acc: 0.512965\n",
      "epoch: 14, Train Loss: 0.111061, Train Acc: 0.510805\n",
      "epoch: 15, Train Loss: 0.111326, Train Acc: 0.510140\n",
      "epoch: 16, Train Loss: 0.110707, Train Acc: 0.510472\n",
      "epoch: 17, Train Loss: 0.110605, Train Acc: 0.510805\n",
      "epoch: 18, Train Loss: 0.110458, Train Acc: 0.510306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m _,pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m _,result \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "example = LeNet().to(device)\n",
    "\n",
    "lr = 0.001\n",
    "epoches = 40\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "loss_list1 = []\n",
    "accuracy_list1 = []\n",
    "optimizer = torch.optim.SGD(example.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "\n",
    "    for img,label,_ in train_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = example(img)\n",
    "        loss = criterion(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        train_acc += acc\n",
    "    loss_list.append(train_loss/len(train_loader))\n",
    "    accuracy_list.append(train_acc/len(train_loader))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}'.format(epoch+1, train_loss/len(train_loader), train_acc/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.571875\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "for img,label,_ in test_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = example(img)\n",
    "\n",
    "\n",
    "\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        test_acc += acc\n",
    "print(test_acc/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, Train Loss: 0.468922, Train Acc: 0.459774\n",
      "epoch: 2, Train Loss: 0.442489, Train Acc: 0.485705\n",
      "epoch: 3, Train Loss: 0.126226, Train Acc: 0.493684\n",
      "epoch: 4, Train Loss: 0.683302, Train Acc: 0.496509\n",
      "epoch: 5, Train Loss: 0.206603, Train Acc: 0.502660\n",
      "epoch: 6, Train Loss: 14.916968, Train Acc: 0.473072\n",
      "epoch: 7, Train Loss: 0.179896, Train Acc: 0.500000\n",
      "epoch: 8, Train Loss: 0.113079, Train Acc: 0.501330\n",
      "epoch: 9, Train Loss: 0.256043, Train Acc: 0.507979\n",
      "epoch: 10, Train Loss: 0.113292, Train Acc: 0.505153\n",
      "epoch: 11, Train Loss: 0.105343, Train Acc: 0.522108\n",
      "epoch: 12, Train Loss: 3.100542, Train Acc: 0.512965\n",
      "epoch: 13, Train Loss: 0.135791, Train Acc: 0.506483\n",
      "epoch: 14, Train Loss: 0.126079, Train Acc: 0.505319\n",
      "epoch: 15, Train Loss: 0.178775, Train Acc: 0.508810\n",
      "epoch: 16, Train Loss: 5.463059, Train Acc: 0.487367\n",
      "epoch: 17, Train Loss: 1.115323, Train Acc: 0.504322\n",
      "epoch: 18, Train Loss: 1.001642, Train Acc: 0.506316\n",
      "epoch: 19, Train Loss: 0.111011, Train Acc: 0.512467\n",
      "epoch: 20, Train Loss: 0.318704, Train Acc: 0.507646\n",
      "epoch: 21, Train Loss: 0.109277, Train Acc: 0.509641\n",
      "epoch: 22, Train Loss: 0.107670, Train Acc: 0.506316\n",
      "epoch: 23, Train Loss: 0.255534, Train Acc: 0.513630\n",
      "epoch: 24, Train Loss: 0.155645, Train Acc: 0.510472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m _,pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m _,result \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "example = vgg16().to(device)\n",
    "\n",
    "lr = 0.0001\n",
    "epoches = 40\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "loss_list1 = []\n",
    "accuracy_list1 = []\n",
    "optimizer = torch.optim.Adam(example.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for img,label,_ in train_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = example(img)\n",
    "        loss = criterion(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        train_acc += acc\n",
    "    loss_list.append(train_loss/len(train_loader))\n",
    "    accuracy_list.append(train_acc/len(train_loader))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}'.format(epoch+1, train_loss/len(train_loader), train_acc/len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5734375\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "for img,label,_ in test_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = example(img)\n",
    "\n",
    "\n",
    "\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        test_acc += acc\n",
    "print(test_acc/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnetexample = torchvision.models.resnet18(pretrained=True)\n",
    "resnetexample.fc = nn.Linear(resnetexample.fc.in_features,20)\n",
    "resnetexample = resnetexample.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, Train Loss: 0.645103, Train Acc: 0.827793\n",
      "epoch: 2, Train Loss: 0.293204, Train Acc: 0.906749\n",
      "epoch: 3, Train Loss: 0.210051, Train Acc: 0.934009\n",
      "epoch: 4, Train Loss: 0.154426, Train Acc: 0.951130\n",
      "epoch: 5, Train Loss: 0.145466, Train Acc: 0.952128\n",
      "epoch: 6, Train Loss: 0.116719, Train Acc: 0.962932\n",
      "epoch: 7, Train Loss: 0.097587, Train Acc: 0.970412\n",
      "epoch: 8, Train Loss: 0.101398, Train Acc: 0.967919\n",
      "epoch: 9, Train Loss: 0.095263, Train Acc: 0.969914\n",
      "epoch: 10, Train Loss: 0.074589, Train Acc: 0.977726\n",
      "epoch: 11, Train Loss: 0.076528, Train Acc: 0.977061\n",
      "epoch: 12, Train Loss: 0.066376, Train Acc: 0.979887\n",
      "epoch: 13, Train Loss: 0.067370, Train Acc: 0.977726\n",
      "epoch: 14, Train Loss: 0.066079, Train Acc: 0.981051\n",
      "epoch: 15, Train Loss: 0.053978, Train Acc: 0.983544\n",
      "epoch: 16, Train Loss: 0.070342, Train Acc: 0.978557\n",
      "epoch: 17, Train Loss: 0.060927, Train Acc: 0.979887\n",
      "epoch: 18, Train Loss: 0.058307, Train Acc: 0.980053\n",
      "epoch: 19, Train Loss: 0.054309, Train Acc: 0.982879\n",
      "epoch: 20, Train Loss: 0.058925, Train Acc: 0.981715\n",
      "epoch: 21, Train Loss: 0.076172, Train Acc: 0.975731\n",
      "epoch: 22, Train Loss: 0.049585, Train Acc: 0.984707\n",
      "epoch: 23, Train Loss: 0.047846, Train Acc: 0.985040\n",
      "epoch: 24, Train Loss: 0.041184, Train Acc: 0.986203\n",
      "epoch: 25, Train Loss: 0.042094, Train Acc: 0.987367\n",
      "epoch: 26, Train Loss: 0.044685, Train Acc: 0.985871\n",
      "epoch: 27, Train Loss: 0.042482, Train Acc: 0.986702\n",
      "epoch: 28, Train Loss: 0.047624, Train Acc: 0.985871\n",
      "epoch: 29, Train Loss: 0.048299, Train Acc: 0.983876\n",
      "epoch: 30, Train Loss: 0.042878, Train Acc: 0.983876\n",
      "epoch: 31, Train Loss: 0.039468, Train Acc: 0.987533\n",
      "epoch: 32, Train Loss: 0.033317, Train Acc: 0.988198\n",
      "epoch: 33, Train Loss: 0.037268, Train Acc: 0.987866\n",
      "epoch: 34, Train Loss: 0.029242, Train Acc: 0.990858\n",
      "epoch: 35, Train Loss: 0.035207, Train Acc: 0.987533\n",
      "epoch: 36, Train Loss: 0.047925, Train Acc: 0.984043\n",
      "epoch: 37, Train Loss: 0.049164, Train Acc: 0.985206\n",
      "epoch: 38, Train Loss: 0.044990, Train Acc: 0.985705\n",
      "epoch: 39, Train Loss: 0.040308, Train Acc: 0.986037\n",
      "epoch: 40, Train Loss: 0.028954, Train Acc: 0.990027\n",
      "epoch: 41, Train Loss: 0.036039, Train Acc: 0.987367\n",
      "epoch: 42, Train Loss: 0.033112, Train Acc: 0.990359\n",
      "epoch: 43, Train Loss: 0.038618, Train Acc: 0.986702\n",
      "epoch: 44, Train Loss: 0.034131, Train Acc: 0.988032\n",
      "epoch: 45, Train Loss: 0.034202, Train Acc: 0.986370\n",
      "epoch: 46, Train Loss: 0.031671, Train Acc: 0.989362\n",
      "epoch: 47, Train Loss: 0.032870, Train Acc: 0.988198\n",
      "epoch: 48, Train Loss: 0.032648, Train Acc: 0.990027\n",
      "epoch: 49, Train Loss: 0.032620, Train Acc: 0.989195\n",
      "epoch: 50, Train Loss: 0.038117, Train Acc: 0.988531\n",
      "epoch: 51, Train Loss: 0.026877, Train Acc: 0.991689\n",
      "epoch: 52, Train Loss: 0.026886, Train Acc: 0.991190\n",
      "epoch: 53, Train Loss: 0.029258, Train Acc: 0.990858\n",
      "epoch: 54, Train Loss: 0.026750, Train Acc: 0.991523\n",
      "epoch: 55, Train Loss: 0.031070, Train Acc: 0.989195\n",
      "epoch: 56, Train Loss: 0.035192, Train Acc: 0.988697\n",
      "epoch: 57, Train Loss: 0.028752, Train Acc: 0.988863\n",
      "epoch: 58, Train Loss: 0.031954, Train Acc: 0.991523\n",
      "epoch: 59, Train Loss: 0.026754, Train Acc: 0.992188\n",
      "epoch: 60, Train Loss: 0.030365, Train Acc: 0.989195\n",
      "epoch: 61, Train Loss: 0.029984, Train Acc: 0.988863\n",
      "epoch: 62, Train Loss: 0.034057, Train Acc: 0.987533\n",
      "epoch: 63, Train Loss: 0.030952, Train Acc: 0.989694\n",
      "epoch: 64, Train Loss: 0.032429, Train Acc: 0.989528\n",
      "epoch: 65, Train Loss: 0.031004, Train Acc: 0.990193\n",
      "epoch: 66, Train Loss: 0.032007, Train Acc: 0.989029\n",
      "epoch: 67, Train Loss: 0.027941, Train Acc: 0.989860\n",
      "epoch: 68, Train Loss: 0.024294, Train Acc: 0.991190\n",
      "epoch: 69, Train Loss: 0.031869, Train Acc: 0.988863\n",
      "epoch: 70, Train Loss: 0.030980, Train Acc: 0.989860\n",
      "epoch: 71, Train Loss: 0.029111, Train Acc: 0.989860\n",
      "epoch: 72, Train Loss: 0.025750, Train Acc: 0.990691\n",
      "epoch: 73, Train Loss: 0.023477, Train Acc: 0.992520\n",
      "epoch: 74, Train Loss: 0.020763, Train Acc: 0.993185\n",
      "epoch: 75, Train Loss: 0.022522, Train Acc: 0.991024\n",
      "epoch: 76, Train Loss: 0.027055, Train Acc: 0.991689\n",
      "epoch: 77, Train Loss: 0.024020, Train Acc: 0.993019\n",
      "epoch: 78, Train Loss: 0.021886, Train Acc: 0.994182\n",
      "epoch: 79, Train Loss: 0.019872, Train Acc: 0.993850\n",
      "epoch: 80, Train Loss: 0.025272, Train Acc: 0.990858\n",
      "epoch: 81, Train Loss: 0.024276, Train Acc: 0.992021\n",
      "epoch: 82, Train Loss: 0.019261, Train Acc: 0.994515\n",
      "epoch: 83, Train Loss: 0.023482, Train Acc: 0.993517\n",
      "epoch: 84, Train Loss: 0.027040, Train Acc: 0.991356\n",
      "epoch: 85, Train Loss: 0.025131, Train Acc: 0.990525\n",
      "epoch: 86, Train Loss: 0.018089, Train Acc: 0.994016\n",
      "epoch: 87, Train Loss: 0.023386, Train Acc: 0.993185\n",
      "epoch: 88, Train Loss: 0.020436, Train Acc: 0.994847\n",
      "epoch: 89, Train Loss: 0.025354, Train Acc: 0.991689\n",
      "epoch: 90, Train Loss: 0.023172, Train Acc: 0.992520\n",
      "epoch: 91, Train Loss: 0.038596, Train Acc: 0.988697\n",
      "epoch: 92, Train Loss: 0.027551, Train Acc: 0.990691\n",
      "epoch: 93, Train Loss: 0.027573, Train Acc: 0.992188\n",
      "epoch: 94, Train Loss: 0.025121, Train Acc: 0.991190\n",
      "epoch: 95, Train Loss: 0.026320, Train Acc: 0.992188\n",
      "epoch: 96, Train Loss: 0.017779, Train Acc: 0.993850\n",
      "epoch: 97, Train Loss: 0.016171, Train Acc: 0.995013\n",
      "epoch: 98, Train Loss: 0.016830, Train Acc: 0.994016\n",
      "epoch: 99, Train Loss: 0.017659, Train Acc: 0.993019\n",
      "epoch: 100, Train Loss: 0.014961, Train Acc: 0.994515\n",
      "epoch: 101, Train Loss: 0.016017, Train Acc: 0.994515\n",
      "epoch: 102, Train Loss: 0.022603, Train Acc: 0.992520\n",
      "epoch: 103, Train Loss: 0.019768, Train Acc: 0.992686\n",
      "epoch: 104, Train Loss: 0.029968, Train Acc: 0.990525\n",
      "epoch: 105, Train Loss: 0.028879, Train Acc: 0.991855\n",
      "epoch: 106, Train Loss: 0.017601, Train Acc: 0.993351\n",
      "epoch: 107, Train Loss: 0.023180, Train Acc: 0.992188\n",
      "epoch: 108, Train Loss: 0.021362, Train Acc: 0.993019\n",
      "epoch: 109, Train Loss: 0.023856, Train Acc: 0.991689\n",
      "epoch: 110, Train Loss: 0.024748, Train Acc: 0.993185\n",
      "epoch: 111, Train Loss: 0.021124, Train Acc: 0.994016\n",
      "epoch: 112, Train Loss: 0.017391, Train Acc: 0.992852\n",
      "epoch: 113, Train Loss: 0.021546, Train Acc: 0.992188\n",
      "epoch: 114, Train Loss: 0.019663, Train Acc: 0.993684\n",
      "epoch: 115, Train Loss: 0.013656, Train Acc: 0.995346\n",
      "epoch: 116, Train Loss: 0.018823, Train Acc: 0.994681\n",
      "epoch: 117, Train Loss: 0.011844, Train Acc: 0.995678\n",
      "epoch: 118, Train Loss: 0.017315, Train Acc: 0.995180\n",
      "epoch: 119, Train Loss: 0.016340, Train Acc: 0.994681\n",
      "epoch: 120, Train Loss: 0.017423, Train Acc: 0.994847\n",
      "epoch: 121, Train Loss: 0.018359, Train Acc: 0.994681\n",
      "epoch: 122, Train Loss: 0.014125, Train Acc: 0.996011\n",
      "epoch: 123, Train Loss: 0.021888, Train Acc: 0.992686\n",
      "epoch: 124, Train Loss: 0.023217, Train Acc: 0.994016\n",
      "epoch: 125, Train Loss: 0.021764, Train Acc: 0.993684\n",
      "epoch: 126, Train Loss: 0.018880, Train Acc: 0.993517\n",
      "epoch: 127, Train Loss: 0.014642, Train Acc: 0.995346\n",
      "epoch: 128, Train Loss: 0.021445, Train Acc: 0.992686\n",
      "epoch: 129, Train Loss: 0.022728, Train Acc: 0.993517\n",
      "epoch: 130, Train Loss: 0.017959, Train Acc: 0.994348\n",
      "epoch: 131, Train Loss: 0.020881, Train Acc: 0.992686\n",
      "epoch: 132, Train Loss: 0.023502, Train Acc: 0.992021\n",
      "epoch: 133, Train Loss: 0.023517, Train Acc: 0.992021\n",
      "epoch: 134, Train Loss: 0.016712, Train Acc: 0.994182\n",
      "epoch: 135, Train Loss: 0.015831, Train Acc: 0.995180\n",
      "epoch: 136, Train Loss: 0.018982, Train Acc: 0.995013\n",
      "epoch: 137, Train Loss: 0.014028, Train Acc: 0.995844\n",
      "epoch: 138, Train Loss: 0.018082, Train Acc: 0.994515\n",
      "epoch: 139, Train Loss: 0.027551, Train Acc: 0.990858\n",
      "epoch: 140, Train Loss: 0.020401, Train Acc: 0.994182\n",
      "epoch: 141, Train Loss: 0.013193, Train Acc: 0.996343\n",
      "epoch: 142, Train Loss: 0.015928, Train Acc: 0.994681\n",
      "epoch: 143, Train Loss: 0.023589, Train Acc: 0.992852\n",
      "epoch: 144, Train Loss: 0.020589, Train Acc: 0.994847\n",
      "epoch: 145, Train Loss: 0.009972, Train Acc: 0.997008\n",
      "epoch: 146, Train Loss: 0.011091, Train Acc: 0.996177\n",
      "epoch: 147, Train Loss: 0.014765, Train Acc: 0.994515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img,label,_ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     15\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(label\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m1\u001b[39m),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     16\u001b[0m     label \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(label,\u001b[38;5;241m20\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32md:\\study\\pytorch\\anaconda\\envs\\test-cv2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\study\\pytorch\\anaconda\\envs\\test-cv2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\study\\pytorch\\anaconda\\envs\\test-cv2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\study\\pytorch\\anaconda\\envs\\test-cv2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36mTinySegData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_train_item(idx)\n",
      "Cell \u001b[1;32mIn[1], line 60\u001b[0m, in \u001b[0;36mTinySegData.get_train_item\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     57\u001b[0m seg_gt \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39masarray(Image\u001b[38;5;241m.\u001b[39mopen(sample[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m     59\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 60\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m127.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m        \u001b[38;5;66;03m# -1~1\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     63\u001b[0m     image \u001b[38;5;241m=\u001b[39m image[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]       \u001b[38;5;66;03m# HWC\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "epoches = 40\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(resnetexample.parameters(),lr=lr)\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "loss_list1 = []\n",
    "accuracy_list1 = []\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    for img,label,_ in train_loader:\n",
    "        label = torch.max(label.flatten(1),dim=1)[0].long()\n",
    "        label = nn.functional.one_hot(label,20).float()\n",
    "        img,label = img.to(device),label.to(device)\n",
    "        output = resnetexample(img)\n",
    "        loss = criterion(output,label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _,pred = output.max(1)\n",
    "        _,result = label.max(1)\n",
    "        num_correct = (pred==result).sum().item()\n",
    "        acc = num_correct / img.shape[0]\n",
    "        train_acc += acc\n",
    "    loss_list.append(train_loss/len(train_loader))\n",
    "    accuracy_list.append(train_acc/len(train_loader))\n",
    "    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}'.format(epoch+1, train_loss/len(train_loader), train_acc/len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-cv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
